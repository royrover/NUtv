import requests
from bs4 import BeautifulSoup
from datetime import datetime
import re
import json
import os
import platform

# === ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£ ===
SYSTEM = platform.system()

if SYSTEM == "Windows":
    SAVE_DIR = os.path.dirname(os.path.abspath(__file__))
elif SYSTEM == "Linux":
    SAVE_DIR = os.path.join(os.getcwd(), "data/sport_rerun")
else:  # Android (Termux)
    SAVE_DIR = "/storage/emulated/0/htdocs/PYTHON/HL UPDATE/sport_rerun"

os.makedirs(SAVE_DIR, exist_ok=True)
json_file = os.path.join(SAVE_DIR, "acdsport_replay.json")

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/237.84.2.178 Safari/537.36",
}

url = "https://acdsport.com/replay.html"

# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤ ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
if os.path.exists(json_file):
    with open(json_file, "r", encoding="utf-8") as f:
        data = json.load(f)
else:
    data = {
        "name": "acdsport replay",
        "image": "https://acdsport.com/acd/social.jpg",
        "url": "",
        "author": "",
        "stations": [],
    }

# ‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ URL ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥
existing_urls = set(item["url"] for item in data["stations"])

# ‡πÄ‡∏Å‡πá‡∏ö URL ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡∏£‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô (‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏ã‡πâ‡∏≥‡πÅ‡∏•‡πâ‡∏ß break)
old_first_url = data["stations"][0]["url"] if data["stations"] else None

new_stations = []
stop_fetch = False

response = requests.get(url, headers=headers)
response.encoding = "utf-8"
response.raise_for_status()

soup = BeautifulSoup(response.text, "html.parser")
all_links = soup.find_all(
    "div", class_="d-flex justify-content-between fixture-page-item active"
)

for link in all_links:
    if stop_fetch:
        break
    try:
        time = link.find("p", class_="time-format").text.strip()
        league = link.find("div", class_="mt-1 tournament").text.strip()
        home_team = link.find("span", class_="name-team name-team-left").text.strip()
        home_img = (
            link.find_all("img", class_="logo-team")[0]["src"]
            if link.find_all("img", class_="logo-team")
            else None
        )
        away_team = link.find("span", class_="name-team name-team-right").text.strip()
        url_link = link.find("div", class_="box-play").a["href"]
        if not url_link:
            continue

        full_url = url_link
        print(f"‚öΩ {time} {league} {home_team} vs {away_team}")
        print(f"üåê ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á: {full_url}")

        response_match = requests.get(full_url, headers=headers)
        response_match.raise_for_status()
        match_html = response_match.text

        match_soup = BeautifulSoup(match_html, "html.parser")
        iframe_src = match_soup.find("iframe", id="iframePlayer").get("src")
        iframe_response = requests.get(iframe_src, headers=headers)
        iframe_response.raise_for_status()
        iframe_html = iframe_response.text

        final_url_match = re.search(r'link:"(http.*?\.m3u8)"', iframe_html)
        if final_url_match:
            final_url = final_url_match.group(1).encode().decode("unicode_escape")

            # ‚úÖ ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏£‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô ‚Üí ‡∏´‡∏¢‡∏∏‡∏î‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            if old_first_url and final_url == old_first_url:
                print("‚èπÔ∏è ‡πÄ‡∏à‡∏≠‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß ‡∏´‡∏¢‡∏∏‡∏î‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
                stop_fetch = True
                break

            if final_url in existing_urls:
                print("‚è© ‡∏Ç‡πâ‡∏≤‡∏° (‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô)")
                continue

            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô
            station_data = {
                "name": f"{time} {league} {home_team} vs {away_team}",
                "image": home_img,
                "url": final_url,
                "userAgent": "Mozilla/5.0 (iPhone; CPU iPhone OS 17_5_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.1 Mobile/15E148 Safari/605.1.15/Clipbox+/2.2.8",
                "referer": "https://acdsport.com/",
            }
            new_stations.append(station_data)
            existing_urls.add(final_url)
            print(f"‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°: {station_data['name']}")
        else:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå m3u8")
    except Exception as e:
        print(f"‚ö†Ô∏è Error: {e}")
        continue

# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô
data["stations"] = new_stations + data["stations"]

# ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
data["author"] = f"update {datetime.now().strftime('%d-%m-%Y %H:%M:%S')}"

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å JSON
with open(json_file, "w", encoding="utf-8") as f:
    json.dump(data, f, indent=4, ensure_ascii=False)
print(json.dumps(data, ensure_ascii=False, indent=2))
print("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢:", json_file)
