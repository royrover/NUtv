import requests
from bs4 import BeautifulSoup
import os
import re
from urllib.parse import urljoin

# ================= CONFIG =================
SYSTEM = platform.system()
if SYSTEM == "Windows":
    SAVE_DIR = os.path.dirname(os.path.abspath(__file__))
else:  # Linux / Termux / GitHub
    SAVE_DIR = os.path.join(os.getcwd(), "data/live_tv")

# ================= CONFIG =================
BASE_URL = "https://inwtv.site/views.php"
LOGIN_URL = "https://inwtv.site/login.php"
USERNAME = user_inw
PASSWORD = pass_inw
HEADERS = {"User-Agent": "Mozilla/5.0"}
M3U8_FOLDER = SAVE_DIR.

os.makedirs(M3U8_FOLDER, exist_ok=True)

# ================= HELPER =================
def sanitize_filename(name):
    """‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡πÑ‡∏°‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå"""
    return re.sub(r'[<>:"/\\|?*]', "_", name)

def login():
    session = requests.Session()
    payload = {"username": USERNAME, "password": PASSWORD, "remember": "1"}
    try:
        res = session.post(LOGIN_URL, data=payload, headers=HEADERS, timeout=10)
        res.raise_for_status()
    except requests.RequestException as e:
        print(f"‚ùå ‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
        return None
    print("‚úÖ ‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    return session

def scrape_channels(session):
    """‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ä‡πà‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏à‡∏≤‡∏Å views.php"""
    try:
        res = session.get(BASE_URL, headers=HEADERS, timeout=10)
        res.raise_for_status()
    except requests.RequestException as e:
        print(f"‚ùå ‡∏î‡∏∂‡∏á {BASE_URL} ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
        return []

    soup = BeautifulSoup(res.text, "html.parser")
    channels = []
    for card in soup.select(".channel-card"):
        title = card.get("data-title", "").strip()
        onclick = card.get("onclick", "")
        id_match = re.search(r"id=(\d+)", onclick)
        if id_match:
            ch_id = id_match.group(1)
            channels.append({
                "title": title,
                "id": ch_id,
                "url": urljoin(BASE_URL, f"viewep.php?id={ch_id}")
            })
    print(f"üîç ‡∏û‡∏ö {len(channels)} ‡∏ä‡πà‡∏≠‡∏á")
    return channels

def scrape_subchannels(session, viewep_url):
    """‡∏î‡∏∂‡∏á sub-channels ‡∏à‡∏≤‡∏Å viewep.php?id=xxx"""
    try:
        res = session.get(viewep_url, headers=HEADERS, timeout=10)
        res.raise_for_status()
    except requests.RequestException as e:
        print(f"‚ùå ‡∏î‡∏∂‡∏á {viewep_url} ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
        return []

    soup = BeautifulSoup(res.text, "html.parser")
    subchannels = []

    for card in soup.select(".channel-card"):
        h5_tag = card.find("h5")
        title = h5_tag.get_text(strip=True) if h5_tag else "NoTitle"
        onclick = card.get("onclick", "")
        id_match = re.search(r"ReadID\((\d+)\)", onclick)
        if id_match:
            sub_id = id_match.group(1)
            subchannels.append({
                "title": title,
                "id": sub_id
            })
    return subchannels

def get_hls_from_check(session, check_id):
    """‡∏î‡∏∂‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå HLS ‡∏à‡∏≤‡∏Å check.php?id=xxx"""
    check_url = f"https://inwtv.site/check.php?id={check_id}"
    try:
        res = session.get(check_url, headers=HEADERS, timeout=10)
        res.raise_for_status()
        data = res.json()
        if "hls" in data and data["hls"]:
            return data["hls"]
        # fallback regex
        m = re.search(r'https?://[^"\']+\.m3u8[^\s"\']*', res.text)
        if m:
            return m.group(0)
    except Exception as e:
        print(f"‚ùå ‡∏î‡∏∂‡∏á HLS {check_id} ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
    return None

# ================= MAIN =================
if __name__ == "__main__":
    session = login()
    if not session:
        exit()

    channels = scrape_channels(session)

    # ‡πÄ‡∏Å‡πá‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ã‡πâ‡∏≥
    name_counter = {}  # ‡∏ô‡∏±‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏ï‡πà‡∏•‡∏∞ safe_title ‡∏ã‡πâ‡∏≥‡∏Å‡∏µ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á
    all_subchannels = []

    for ch in channels:
        subchannels = scrape_subchannels(session, ch["url"])
        for sub in subchannels:
            all_subchannels.append(sub)
            safe_title = sanitize_filename(sub["title"]).replace("Play ", " ")
            name_counter[safe_title] = name_counter.get(safe_title, 0) + 1

    # ‡πÄ‡∏Å‡πá‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏ã‡πâ‡∏≥
    created_counter = {}

    for sub in all_subchannels:
        hls = get_hls_from_check(session, sub["id"])
        if not hls:
            continue

        safe_title = sanitize_filename(sub["title"])
        count = name_counter[safe_title]

        # ‡∏ñ‡πâ‡∏≤‡∏ã‡πâ‡∏≥ >1 ‡πÉ‡∏ä‡πâ _1, _2
        if count > 1:
            created_counter[safe_title] = created_counter.get(safe_title, 0) + 1
            idx = created_counter[safe_title]
            filename = f"{M3U8_FOLDER}/{safe_title}_{idx}.m3u8"
        else:
            filename = f"{M3U8_FOLDER}/{safe_title}.m3u8"

        os.makedirs(M3U8_FOLDER, exist_ok=True)
        with open(filename, "w", encoding="utf-8") as f:
            f.write("#EXTM3U\n")
            f.write("#EXT-X-VERSION:3\n")
            f.write("#EXT-X-STREAM-INF:PROGRAM-ID=1,BANDWIDTH=20000000\n")
            f.write(f"{hls}\n")
        print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå M3U8: {filename}")


