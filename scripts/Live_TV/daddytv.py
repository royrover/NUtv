import aiohttp
import asyncio
import json, re, os, platform
from bs4 import BeautifulSoup
from datetime import datetime

# ================= CONFIG =================
SYSTEM = platform.system()
if SYSTEM == "Windows":
    SAVE_DIR = os.path.dirname(os.path.abspath(__file__))
else:  # Linux / Termux / GitHub
    SAVE_DIR = os.path.join(os.getcwd(), "data/live_tv")

os.makedirs(SAVE_DIR, exist_ok=True)
json_file = os.path.join(SAVE_DIR, "daddytv.json")
m3u_file = os.path.join(SAVE_DIR, "daddytv.m3u")

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
                  " AppleWebKit/537.36 (KHTML, like Gecko)"
                  " Chrome/91.0.4472.124 Safari/537.36"
}

# =============== FILE FUNCTIONS ===============
def load_old_data(path):
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return None

def save_new_data(path, data, m3u_path=None):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print("üìÅ ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢:", path)

    if m3u_path:
        m3u = "#EXTM3U\n"
        for s in data.get("stations", []):
            m3u += (
                f'#EXTINF:-1 tvg-logo="{s["image"]}" group-title="Live TV",{s["name"]}\n'
                f'#EXTVLCOPT:http-referrer={s["referer"]}\n'
                f'#EXTVLCOPT:http-user-agent={s["userAgent"]}\n'
                f'{s["url"]}\n'
            )
        try:
            with open(m3u_path, "w", encoding="utf-8") as f:
                f.write(m3u)
            print("‚úÖ M3U update done, ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ä‡πà‡∏≠‡∏á:", len(data.get("stations", [])))
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô M3U ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")

        if not data.get("stations"):
            print("‚ö†Ô∏è stations ‡∏ß‡πà‡∏≤‡∏á ‚Äî M3U ‡∏à‡∏∞‡∏°‡∏µ‡πÅ‡∏ï‡πà header")


# =============== NETWORK HELPERS ===============
async def fetch_with_retry(session, url, retries=3, delay=5):
    for attempt in range(retries):
        try:
            async with session.get(url, headers=headers, timeout=30) as resp:
                if resp.status == 403:
                    print("üö´ Forbidden:", url)
                    return None
                resp.raise_for_status()
                return await resp.text()
        except aiohttp.ClientError as e:
            print(f"‚ö†Ô∏è Attempt {attempt+1}/{retries} failed: {e}")
            if attempt < retries - 1:
                await asyncio.sleep(delay)
    return None

async def get_referer(session):
    url = "https://daddylivestream.com/24-7-channels.php"
    html = await fetch_with_retry(session, url)
    if not html:
        return None, None, None

    soup = BeautifulSoup(html, "html.parser")
    container = soup.find("div", class_="grid-container")
    if not container:
        return None, None, None

    first_channel = container.find("a")
    if not first_channel:
        return None, None, None

    try:
        channel_id = re.search(r"stream-(\d+)\.php", first_channel.get("href")).group(1)
        embed_url = f"https://dlhd.dad/embed/stream-{channel_id}.php"
        html_embed = await fetch_with_retry(session, embed_url)
        if not html_embed:
            return None, None, None
        match = re.search(r"https.*?daddyhd\.php\?id=\d+", html_embed)
        if not match:
            return None, None, None
        request_url = match.group(0)
        base_host = re.search(r"https://[^/]+", request_url).group(0)
        return request_url, base_host, html
    except Exception:
        return None, None, None

# =============== SCRAPE FUNCTION ===============
async def scrape_channel(session, ch, idx, total, headers_ref, base_host, data, sem):
    async with sem:
        name = ch.text.strip()
        href = ch.get("href", "")
        if not href or "18+" in name:
            print(f"‚ùå ‡∏Ç‡πâ‡∏≤‡∏°‡∏ä‡πà‡∏≠‡∏á {name}")
            return

        if any(s["name"] == name for s in data["stations"]):
            print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏ä‡πà‡∏≠‡∏á‡∏ã‡πâ‡∏≥ {name}")
            return  # skip duplicate

        try:
            channel_id = re.search(r"stream-(\d+)\.php", href).group(1)
        except:
            print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö ID ‡∏ä‡πà‡∏≠‡∏á {name}")
            return

        print(f"{idx}/{total} üì∫ {name} (ID:{channel_id})")

        embed_url = f"https://dlhd.dad/embed/stream-{channel_id}.php"
        html_embed = await fetch_with_retry(session, embed_url)
        if not html_embed:
            print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á embed {name}")
            return
        match = re.search(r"https.*?daddyhd\.php\?id=\d+", html_embed)
        if not match:
            print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö URL daddyhd.php {name}")
            return
        request_url = match.group(0)

        try:
            async with session.get(request_url, headers=headers_ref) as resp:
                if resp.status != 200:
                    print(f"‚ö†Ô∏è HTTP {resp.status} {name}")
                    return
                html_content = await resp.text()
                match_key = re.search(r'const CHANNEL_KEY="([^"]+)";', html_content)
                if not match_key:
                    print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö CHANNEL_KEY {name}")
                    return
                channelKey = match_key.group(1)
        except Exception as e:
            print(f"‚ö†Ô∏è Error fetch key {name}: {e}")
            return

        lookup_url = f"{base_host}/server_lookup.php?channel_id={channelKey}"
        try:
            async with session.get(lookup_url, headers=headers_ref) as lresp:
                if lresp.status != 200:
                    print(f"‚ö†Ô∏è HTTP lookup {lresp.status} {name}")
                    return
                js = await lresp.json()
                sk = js.get("server_key", "")
                if not sk:
                    print(f"‚ö†Ô∏è server_key ‡∏ß‡πà‡∏≤‡∏á {name}")
                    return
                if sk == "top1/cdn":
                    m3u8_url = f"https://top1.newkso.ru/top1/cdn/{channelKey}/mono.m3u8"
                else:
                    m3u8_url = f"https://{sk}new.newkso.ru/{sk}/{channelKey}/mono.m3u8"
        except Exception as e:
            print(f"‚ö†Ô∏è Error lookup {name}: {e}")
            return

        data["stations"].append({
            "name": name,
            "image": data["image"],
            "url": m3u8_url,
            "id": channel_id,
            "referer": base_host + "/",
            "userAgent": "Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) "
                         "AppleWebKit/605.1.15 (KHTML, like Gecko) "
                         "Version/13.0.3 Mobile/15E148 Safari/604.1",
            "playInNatPlayer": "true",
        })
        print("‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡πà‡∏≠‡∏á:", name)

# =============== MAIN ===============
async def main():
    today = datetime.now().strftime("%Y-%m-%d %H:%M")
    old_data = load_old_data(json_file)

    async with aiohttp.ClientSession() as session:
        request_url, base_host, html = await get_referer(session)
        if not base_host:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö referer")
            return

        # ‡∏ï‡∏£‡∏ß‡∏à referer ‡πÄ‡∏î‡∏¥‡∏°
        if old_data and old_data.get("stations"):
            if old_data["stations"][0].get("referer", "") == base_host + "/":
                print("‚úÖ referer ‡∏¢‡∏±‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‚Üí ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤ + update author")
                old_data["author"] = f"update {today}"
                save_new_data(json_file, old_data, m3u_file)
                return

        print("üîÅ ‡πÄ‡∏£‡∏¥‡πà‡∏° scrape ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î...")
        soup = BeautifulSoup(html, "html.parser")
        container = soup.find("div", class_="grid-container")

        data = {
            "name": "‡∏ä‡πà‡∏≠‡∏á‡∏™‡∏î thedaddy",
            "image": "https://www.cg4tv.com/animation/stock-images-3d/world-map-animated-background.jpg",
            "url": "",
            "author": f"update {today}",
            "stations": [],
        }

        headers_ref = headers.copy()
        headers_ref["Referer"] = base_host + "/"
        channels = container.find_all("a")
        total = len(channels)
        sem = asyncio.Semaphore(8)  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î async 8 ‡∏á‡∏≤‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô

        tasks = [
            scrape_channel(session, ch, idx, total, headers_ref, base_host, data, sem)
            for idx, ch in enumerate(channels, 1)
        ]
        await asyncio.gather(*tasks)

        print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(data['stations'])}")
        if not data["stations"]:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡πÉ‡∏î‡∏î‡∏∂‡∏á‡πÑ‡∏î‡πâ ‚Äî ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå")
            return

        save_new_data(json_file, data, m3u_file)

# =============== RUN ===============
if __name__ == "__main__":
    asyncio.run(main())
